<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>V's Blog</title><link>https://blog.witter.top</link><description>自有清风常载鹤，从无猜意不惊鸥。</description><copyright>V's Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://img.witter.top/file/b037878209903b7d5bb17.jpg</url><title>avatar</title><link>https://blog.witter.top</link></image><lastBuildDate>Tue, 29 Oct 2024 06:01:41 +0000</lastBuildDate><managingEditor>V's Blog</managingEditor><ttl>60</ttl><webMaster>V's Blog</webMaster><item><title>Docker 配置拉取镜像代理</title><link>https://blog.witter.top/post/Docker%20-pei-zhi-la-qu-jing-xiang-dai-li.html</link><description>&gt; 对 `docker daemon` 配置额外的环境变量，来使 `docker pull` 指令经过网络代理访问&#13;
&gt;&#13;
&gt; 参考文档：[DockerDocs:Daemon proxy configuration](https://docs.docker.com/engine/daemon/proxy/#environment-variables)&#13;
&#13;
```shell&#13;
# 使用systemd的系统中，添加额外的配置文件&#13;
mkdir -p /etc/systemd/system/docker.service.d&#13;
&#13;
cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/docker.service.d/http-proxy.conf&#13;
[Service]&#13;
Environment=HTTP_PROXY=http://xxxx:41001&#13;
Environment=HTTPS_PROXY=http://xxxx:41001&#13;
Environment='NO_PROXY=localhost,127.0.0.1,xxxx'&#13;
EOF&#13;
&#13;
systemctl daemon-reload&#13;
systemctl reload docker&#13;
systemctl restart docker&#13;
```。</description><guid isPermaLink="true">https://blog.witter.top/post/Docker%20-pei-zhi-la-qu-jing-xiang-dai-li.html</guid><pubDate>Tue, 29 Oct 2024 05:58:14 +0000</pubDate></item><item><title>使用 Nginx 过滤请求host，达到白名单的效果</title><link>https://blog.witter.top/post/shi-yong-%20Nginx%20-guo-lv-qing-qiu-host%EF%BC%8C-da-dao-bai-ming-dan-de-xiao-guo.html</link><description>```nginx&#13;
server {&#13;
    listen xxx;&#13;
    server_name xxx.xxx.cn;&#13;
&#13;
    location / {&#13;
        # 设置变量，默认不允许任何请求通过&#13;
        set $allowed 0;&#13;
	# 匹配允许访问的域名（正则匹配）&#13;
        if ($http_host ~* 'steamcommunity.com|steampowered.com|steamstatic.com|api.steampowered.com|repo.hex.pm|npm.taobao.org|index.ruby-china.com|gems.ruby-china.com|mirrors.cloud.tencent.com|github.com|epicgames.com') {&#13;
            set $allowed 1;&#13;
        }&#13;
&#13;
        if ($allowed = 0) {&#13;
            return 403;&#13;
        }&#13;
&#13;
        proxy_pass http://127.0.0.1:xxx;&#13;
        proxy_set_header Host $host;&#13;
        proxy_set_header X-Real-IP $remote_addr;&#13;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&#13;
        proxy_set_header X-Forwarded-Proto $scheme;&#13;
    }&#13;
}&#13;
```&#13;
&#13;
。</description><guid isPermaLink="true">https://blog.witter.top/post/shi-yong-%20Nginx%20-guo-lv-qing-qiu-host%EF%BC%8C-da-dao-bai-ming-dan-de-xiao-guo.html</guid><pubDate>Thu, 12 Sep 2024 07:25:32 +0000</pubDate></item><item><title>Linux部分service的作用</title><link>https://blog.witter.top/post/Linux-bu-fen-service-de-zuo-yong.html</link><description>​	1.	**chrony.service**：用于同步系统时间，通常与 NTP（网络时间协议）服务器通信，以确保系统时钟的准确性。</description><guid isPermaLink="true">https://blog.witter.top/post/Linux-bu-fen-service-de-zuo-yong.html</guid><pubDate>Thu, 22 Aug 2024 05:50:37 +0000</pubDate></item><item><title>Nginx-error accept4() failed (24: Too many open files) 问题</title><link>https://blog.witter.top/post/Nginx-error%20accept4%28%29%20failed%20%2824-%20Too%20many%20open%20files%29%20-wen-ti.html</link><description>**网络结构：** 公网-&gt;Nginx-&gt;后端websocket服务&#13;
&#13;
**报错日志：**`2024/07/24 14:03:08 [crit] 708160#0: accept4() failed (24: Too many open files)`&#13;
&#13;
**解决方法：**&#13;
&#13;
1. **查看限制数量**&#13;
&#13;
   ```shell&#13;
   # 下面已经是修改过的配置&#13;
   ulimit -a&#13;
   real-time non-blocking time  (microseconds, -R) unlimited&#13;
   core file size              (blocks, -c) 0&#13;
   data seg size               (kbytes, -d) unlimited&#13;
   scheduling priority                 (-e) 0&#13;
   file size                   (blocks, -f) unlimited&#13;
   pending signals                     (-i) 30297&#13;
   max locked memory           (kbytes, -l) 974046&#13;
   max memory size             (kbytes, -m) unlimited&#13;
   open files                          (-n) 65535    # 主要看这里打开的句柄数量上限(文件数)&#13;
   pipe size                (512 bytes, -p) 8&#13;
   POSIX message queues         (bytes, -q) 819200&#13;
   real-time priority                  (-r) 0&#13;
   stack size                  (kbytes, -s) 8192&#13;
   cpu time                   (seconds, -t) unlimited&#13;
   max user processes                  (-u) 30297&#13;
   virtual memory              (kbytes, -v) unlimited&#13;
   file locks                          (-x) unlimited&#13;
   ```&#13;
&#13;
2. **查看当前系统的连接数量**&#13;
&#13;
   &gt; 1. **TIME_WAIT**：表示连接已经关闭，但TCP还没有完全删除连接，因为需要确保所有数据包都已经被正确接收和处理。</description><guid isPermaLink="true">https://blog.witter.top/post/Nginx-error%20accept4%28%29%20failed%20%2824-%20Too%20many%20open%20files%29%20-wen-ti.html</guid><pubDate>Tue, 30 Jul 2024 06:22:07 +0000</pubDate></item><item><title>Kubernetes集群&amp;证书升级</title><link>https://blog.witter.top/post/Kubernetes-ji-qun-%26-zheng-shu-sheng-ji.html</link><description>&#13;
### 集群升级&#13;
&#13;
&gt; **[官网升级步骤](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)，注意阅读升级说明**&#13;
&#13;
#### 1.更改所有主机软件包存储库地址&#13;
&#13;
&gt; 这里使用的是[清华镜像源地址](https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/)，宿主机使用的包管理器为apt，在以下 URL 中，所有仓库的公钥均相同，只需要将仓库地址中的 `v1.28` 修改为所需的版本&#13;
&#13;
```shell&#13;
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg&#13;
&#13;
vim /etc/apt/sources.list.d/kubernetes.list&#13;
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.tuna.tsinghua.edu.cn/kubernetes/core:/stable:/v1.27/deb/ /&#13;
# deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.tuna.tsinghua.edu.cn/kubernetes/addons:/cri-o:/stable:/v1.27/deb/ /&#13;
```&#13;
&#13;
#### 2.确定要升级到哪个版本&#13;
&#13;
```shell&#13;
# Find the latest 1.30 version in the list.&#13;
# It should look like 1.30.x-*, where x is the latest patch.&#13;
sudo apt update&#13;
sudo apt-cache madison kubeadm&#13;
```&#13;
&#13;
#### 3.升级control-plane节点（kubeadm）&#13;
&#13;
&gt; 该节点必须具有`/etc/kubernetes/admin.conf`文件，**v1.28版本是一个分界点，之前的版本会立即升级所有插件（包括coredns和kube-proxy），需要注意是否还有未升级的其他control-plane节点**&#13;
&#13;
```shell&#13;
# replace x in 1.30.x-* with the latest patch version&#13;
# 此时目标升级至 v1.27.16 需替换下面的版本&#13;
sudo apt-mark unhold kubeadm &amp;&amp; \&#13;
sudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm='1.30.x-*' &amp;&amp; \&#13;
sudo apt-mark hold kubeadm&#13;
&#13;
# 验证是否为预期版本&#13;
kubeadm version&#13;
&#13;
# 验证升级计划&#13;
kubeadm upgrade plan&#13;
	# 以下为输出内容&#13;
[upgrade/config] Making sure the configuration is correct:&#13;
[upgrade/config] Reading configuration from the cluster...&#13;
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'&#13;
[preflight] Running pre-flight checks.&#13;
[upgrade] Running cluster health checks&#13;
[upgrade] Fetching available versions to upgrade to&#13;
[upgrade/versions] Cluster version: v1.27.7&#13;
[upgrade/versions] kubeadm version: v1.27.16&#13;
I0728 14:52:53.611082 1225480 version.go:256] remote version is much newer: v1.30.3; falling back to: stable-1.27&#13;
[upgrade/versions] Target version: v1.27.16&#13;
[upgrade/versions] Latest version in the v1.27 series: v1.27.16&#13;
&#13;
Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':&#13;
COMPONENT   CURRENT       TARGET&#13;
kubelet     2 x v1.27.4   v1.27.16&#13;
&#13;
Upgrade to the latest version in the v1.27 series:&#13;
&#13;
COMPONENT                 CURRENT   TARGET&#13;
kube-apiserver            v1.27.7   v1.27.16&#13;
kube-controller-manager   v1.27.7   v1.27.16&#13;
kube-scheduler            v1.27.7   v1.27.16&#13;
kube-proxy                v1.27.7   v1.27.16&#13;
CoreDNS                   v1.10.1   v1.10.1&#13;
etcd                      3.5.7-0   3.5.12-0&#13;
&#13;
You can now apply the upgrade by executing the following command:&#13;
&#13;
        kubeadm upgrade apply v1.27.16&#13;
&#13;
_____________________________________________________________________&#13;
&#13;
&#13;
The table below shows the current state of component configs as understood by this version of kubeadm.&#13;
Configs that have a 'yes' mark in the 'MANUAL UPGRADE REQUIRED' column require manual config upgrade or&#13;
resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually&#13;
upgrade to is denoted in the 'PREFERRED VERSION' column.&#13;
&#13;
API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED&#13;
kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no&#13;
kubelet.config.k8s.io     v1beta1           v1beta1             no&#13;
_____________________________________________________________________&#13;
&#13;
&#13;
# 执行对应的升级命令&#13;
kubeadm upgrade apply v1.27.16&#13;
	# 以下为输出内容&#13;
[upgrade/config] Making sure the configuration is correct:&#13;
[upgrade/config] Reading configuration from the cluster...&#13;
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'&#13;
[preflight] Running pre-flight checks.&#13;
[upgrade] Running cluster health checks&#13;
[upgrade/version] You have chosen to change the cluster version to 'v1.27.16'&#13;
[upgrade/versions] Cluster version: v1.27.7&#13;
[upgrade/versions] kubeadm version: v1.27.16&#13;
[upgrade] Are you sure you want to proceed? [y/N]: y&#13;
[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster&#13;
[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection&#13;
[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'&#13;
W0728 14:54:24.900816 1226355 checks.go:835] detected that the sandbox image 'registry.aliyuncs.com/google_containers/pause:3.6' of the container runtime is inconsistent with that used by kubeadm. It is recommended that using 'registry.aliyuncs.com/google_containers/pause:3.9' as the CRI sandbox image.&#13;
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version 'v1.27.16' (timeout: 5m0s)...&#13;
[upgrade/etcd] Upgrading to TLS for etcd&#13;
[upgrade/staticpods] Preparing for 'etcd' upgrade&#13;
[upgrade/staticpods] Renewing etcd-server certificate&#13;
[upgrade/staticpods] Renewing etcd-peer certificate&#13;
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate&#13;
[upgrade/staticpods] Moved new manifest to '/etc/kubernetes/manifests/etcd.yaml' and backed up old manifest to '/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-07-28-14-54-44/etcd.yaml'&#13;
[upgrade/staticpods] Waiting for the kubelet to restart the component&#13;
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)&#13;
[apiclient] Found 1 Pods for label selector component=etcd&#13;
[upgrade/staticpods] Component 'etcd' upgraded successfully!&#13;
[upgrade/etcd] Waiting for etcd to become available&#13;
[upgrade/staticpods] Writing new Static Pod manifests to '/etc/kubernetes/tmp/kubeadm-upgraded-manifests97929804'&#13;
[upgrade/staticpods] Preparing for 'kube-apiserver' upgrade&#13;
[upgrade/staticpods] Renewing apiserver certificate&#13;
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate&#13;
[upgrade/staticpods] Renewing front-proxy-client certificate&#13;
[upgrade/staticpods] Renewing apiserver-etcd-client certificate&#13;
[upgrade/staticpods] Moved new manifest to '/etc/kubernetes/manifests/kube-apiserver.yaml' and backed up old manifest to '/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-07-28-14-54-44/kube-apiserver.yaml'&#13;
[upgrade/staticpods] Waiting for the kubelet to restart the component&#13;
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)&#13;
[apiclient] Found 1 Pods for label selector component=kube-apiserver&#13;
[upgrade/staticpods] Component 'kube-apiserver' upgraded successfully!&#13;
[upgrade/staticpods] Preparing for 'kube-controller-manager' upgrade&#13;
[upgrade/staticpods] Renewing controller-manager.conf certificate&#13;
[upgrade/staticpods] Moved new manifest to '/etc/kubernetes/manifests/kube-controller-manager.yaml' and backed up old manifest to '/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-07-28-14-54-44/kube-controller-manager.yaml'&#13;
[upgrade/staticpods] Waiting for the kubelet to restart the component&#13;
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)&#13;
[apiclient] Found 1 Pods for label selector component=kube-controller-manager&#13;
[upgrade/staticpods] Component 'kube-controller-manager' upgraded successfully!&#13;
[upgrade/staticpods] Preparing for 'kube-scheduler' upgrade&#13;
[upgrade/staticpods] Renewing scheduler.conf certificate&#13;
[upgrade/staticpods] Moved new manifest to '/etc/kubernetes/manifests/kube-scheduler.yaml' and backed up old manifest to '/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-07-28-14-54-44/kube-scheduler.yaml'&#13;
[upgrade/staticpods] Waiting for the kubelet to restart the component&#13;
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)&#13;
[apiclient] Found 1 Pods for label selector component=kube-scheduler&#13;
[upgrade/staticpods] Component 'kube-scheduler' upgraded successfully!&#13;
[upload-config] Storing the configuration used in ConfigMap 'kubeadm-config' in the 'kube-system' Namespace&#13;
[kubelet] Creating a ConfigMap 'kubelet-config' in namespace kube-system with the configuration for the kubelets in the cluster&#13;
[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config2956970837/config.yaml&#13;
[kubelet-start] Writing kubelet configuration to file '/var/lib/kubelet/config.yaml'&#13;
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes&#13;
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials&#13;
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token&#13;
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster&#13;
[addons] Applied essential addon: CoreDNS&#13;
[addons] Applied essential addon: kube-proxy&#13;
&#13;
[upgrade/successful] SUCCESS! Your cluster was upgraded to 'v1.27.16'. Enjoy!&#13;
&#13;
[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.&#13;
&#13;
# 升级CNI驱动插件，此时使用的是flannel，无需升级即可兼容；并且CNI驱动作为DS运行，无需在其他控制平面节点上执行此步骤&#13;
# 如果有其他控制平面节点，需要执行&#13;
sudo kubeadm upgrade node&#13;
```&#13;
&#13;
#### 4.升级control-plane节点（kubelet &amp; kubectl）&#13;
&#13;
```shell&#13;
# 腾空该节点 xxx 为控制平面的节点名称&#13;
kubectl drain xxx --ignore-daemonsets&#13;
	# 下面为输出内容 省略了不关键的输出内容&#13;
node/xxx already cordoned&#13;
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-mkn4j, kube-system/kube-proxy-8mf4k, loggie/loggie-tzxtz&#13;
evicting pod xxx&#13;
&#13;
.....&#13;
&#13;
pod/xxx evicted&#13;
node/xxx drained&#13;
&#13;
# 升级kubectl和kubelet&#13;
# 用最新的补丁版本替换 1.30.x-* 中的 x&#13;
sudo apt-mark unhold kubelet kubectl &amp;&amp; \&#13;
sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet='1.30.x-*' kubectl='1.30.x-*' &amp;&amp; \&#13;
sudo apt-mark hold kubelet kubectl&#13;
# 重启&#13;
sudo systemctl daemon-reload&#13;
sudo systemctl restart kubelet&#13;
# 解除节点保护&#13;
# 将 &lt;node-to-uncordon&gt; 替换为你的节点名称&#13;
kubectl uncordon &lt;node-to-uncordon&gt;&#13;
```&#13;
&#13;
#### 5.升级工作节点&#13;
&#13;
```shell&#13;
# 将 1.30.x-* 中的 x 替换为最新的补丁版本&#13;
sudo apt-mark unhold kubeadm &amp;&amp; \&#13;
sudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm='1.30.x-*' &amp;&amp; \&#13;
sudo apt-mark hold kubeadm&#13;
sudo kubeadm upgrade node&#13;
# 腾空节点&#13;
kubectl drain xxx --ignore-daemonsets&#13;
# 重启&#13;
sudo systemctl daemon-reload&#13;
sudo systemctl restart kubelet&#13;
# 在控制平面节点上执行此命令&#13;
# 将 &lt;node-to-uncordon&gt; 替换为你的节点名称&#13;
kubectl uncordon &lt;node-to-uncordon&gt;&#13;
```&#13;
&#13;
&#13;
&#13;
### 证书手动更新&#13;
&#13;
&gt; **[官网升级步骤](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-preparation-of-component-credentials)，需要查看文档中的警告和笔记**，当集群升级后，证书会自行更新&#13;
&#13;
#### 1.检查证书是否过期&#13;
&#13;
```shell&#13;
kubeadm certs check-expiration&#13;
```&#13;
&#13;
#### 2.续订证书&#13;
&#13;
```shell&#13;
kubeadm certs renew all&#13;
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&#13;
sudo chown $(id -u):$(id -g) $HOME/.kube/config&#13;
```&#13;
&#13;
#### 3.当kubelet客户端证书轮换失败&#13;
&#13;
&gt; kube-apiserver报错 `x509: certificate has expired or is not yet valid`&#13;
&#13;
```shell&#13;
# 从故障节点备份和删除 /etc/kubernetes/kubelet.conf 和 /var/lib/kubelet/pki/kubelet-client*&#13;
# 在集群中具有 /etc/kubernetes/pki/ca.key 的正常工作的控制平面节点上执行($NODE为故障节点的名称)&#13;
kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE &gt; kubelet.conf&#13;
# 复制生成的kubelet.conf到/etc/kubernetes/kubelet.conf&#13;
# 在故障节点上执行后，/var/lib/kubelet/pki/kubelet-client-current.pem会重新创建&#13;
systemctl restart kubelet&#13;
# 手动编辑/etc/kubernetes/kubelet.conf 将生成的 client-certificate-data 和 client-key-data 替换为：&#13;
client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem&#13;
client-key: /var/lib/kubelet/pki/kubelet-client-current.pem&#13;
# 重新启动kubelet&#13;
systemctl restart kubelet&#13;
```&#13;
&#13;
#### 4.更新后处理&#13;
&#13;
```shell&#13;
# 重建 apiserver etcd controller-manager scheduler&#13;
kubectl -n kube-system delete po etcd-xxx kube-apiserver-xxx kube-controller-manager-xxx kube-scheduler-xxx&#13;
# 查看对应pod日志，如果仍然报错：x509: certificate has expired or is not yet valid，则：&#13;
# 临时将清单文件从 /etc/kubernetes/manifests/ 移除并等待 20 秒，并再次重建pod&#13;
# 之后你可以将文件移回去，kubelet 可以完成 Pod 的重建，而组件的证书更新操作也得以完成&#13;
```&#13;
&#13;
&#13;
&#13;
。</description><guid isPermaLink="true">https://blog.witter.top/post/Kubernetes-ji-qun-%26-zheng-shu-sheng-ji.html</guid><pubDate>Mon, 29 Jul 2024 03:18:45 +0000</pubDate></item><item><title>redis 小记</title><link>https://blog.witter.top/post/redis%20-xiao-ji.html</link><description>## 计算KEYS占用内存大小&#13;
&#13;
```lua&#13;
-- 初始化游标为 '0'，用于 SCAN 命令的初始值&#13;
local cursor = '0'&#13;
-- 创建一个空表 result，用于存储结果&#13;
local result = {}&#13;
-- 使用 repeat 循环，直到游标返回 '0' 表示扫描结束&#13;
repeat&#13;
    -- 使用 SCAN 命令逐步获取键，SCAN 命令返回一个表，第一个元素是新的游标，第二个元素是键的列表&#13;
    local res = redis.call('SCAN', cursor)&#13;
    -- 更新游标为新的游标值，以便在下一次迭代中继续扫描&#13;
    cursor = res[1]&#13;
    -- 获取键的列表&#13;
    local keys = res[2]&#13;
    -- 遍历每个键&#13;
    for _, key in ipairs(keys) do&#13;
        -- 使用 MEMORY USAGE 命令获取键的内存使用情况&#13;
        local memory = redis.call('MEMORY', 'USAGE', key)&#13;
        -- 将键和其内存使用情况格式化为字符串，并插入到结果表中&#13;
        table.insert(result, key .. ': ' .. memory .. ' bytes')&#13;
    end&#13;
    -- 添加延迟以降低 Redis 的 CPU 占用率，单位为秒&#13;
    redis.call('DEBUG', 'SLEEP', 0.1)    &#13;
-- 继续循环，直到游标为 '0' 表示扫描结束&#13;
until cursor == '0'&#13;
-- 返回结果表&#13;
return result&#13;
```&#13;
&#13;
**使用redis-cli**&#13;
&#13;
```shell&#13;
redis-cli -h xxx -p 6379 -a $(cat ./pwd) --eval mem.lua &gt; mem_result.txt&#13;
cat mem_result.txt |awk -F ':' '{print $1 $2 $NF}'|uniq -c | sort -nr &gt; redis.txt&#13;
```&#13;
&#13;
。</description><guid isPermaLink="true">https://blog.witter.top/post/redis%20-xiao-ji.html</guid><pubDate>Mon, 15 Jul 2024 10:58:25 +0000</pubDate></item><item><title>systemd守护进程文件</title><link>https://blog.witter.top/post/systemd-shou-hu-jin-cheng-wen-jian.html</link><description>## 一些systemd指令&#13;
&#13;
1. 列出开机自启的进程：`systemctl list-unit-files --type=service --state=enabled`&#13;
2. 列出当前正在运行的进程：`systemctl list-units --type=service --state=running`&#13;
&#13;
## Nginx&#13;
&#13;
```ini&#13;
[Unit]&#13;
Description=The NGINX HTTP and reverse proxy server&#13;
After=network.target&#13;
&#13;
[Service]&#13;
Type=forking&#13;
PIDFile=/var/run/nginx.pid&#13;
ExecStartPre=/usr/sbin/nginx -t -c /usr/local/openresty/nginx/conf/nginx.conf&#13;
ExecStart=/usr/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf&#13;
ExecReload=/usr/sbin/nginx -s reload&#13;
ExecStop=/usr/sbin/nginx -s quit&#13;
PrivateTmp=true&#13;
&#13;
[Install]&#13;
WantedBy=multi-user.target&#13;
```&#13;
&#13;
## Prometheus&#13;
&#13;
```ini&#13;
[Unit]&#13;
Description=Prometheus service&#13;
After=network.target&#13;
&#13;
[Service]&#13;
User=root&#13;
Type=simple&#13;
ExecStart=/home/sonkwo/monitor/prometheus/prometheus \&#13;
--web.listen-address=0.0.0.0:9090 \&#13;
--storage.tsdb.path=/etc/prometheus/data \&#13;
--config.file=/etc/prometheus/prometheus.yml \&#13;
--storage.tsdb.retention.time=60d \&#13;
--web.enable-lifecycle \&#13;
--web.enable-admin-api&#13;
ExecReload=/bin/kill -s HUP $MAINPID&#13;
ExecStop=/bin/kill -s QUIT $MAINPID&#13;
Restart=on-failure&#13;
&#13;
[Install]&#13;
WantedBy=multi-user.target&#13;
```&#13;
&#13;
。</description><guid isPermaLink="true">https://blog.witter.top/post/systemd-shou-hu-jin-cheng-wen-jian.html</guid><pubDate>Mon, 15 Jul 2024 06:18:05 +0000</pubDate></item><item><title>MySQL的备份/恢复</title><link>https://blog.witter.top/post/MySQL-de-bei-fen---hui-fu.html</link><description>&#13;
### 备份&#13;
&#13;
**mysqldump**&#13;
&#13;
**安装：**存储库下载[[地址](https://dev.mysql.com/downloads/)](https://dev.mysql.com/downloads/)，选择对应的包管理器&#13;
&#13;
备份多个数据库脚本：&#13;
&#13;
```shell&#13;
#!/bin/bash&#13;
&#13;
# 配置部分&#13;
DB_HOST='localhost'&#13;
DB_USER='your_username'&#13;
DB_PASS='your_password'&#13;
DATABASES=('database1' 'database2' 'database3')  # 需要备份的数据库列表&#13;
BACKUP_DIR='/path/to/your/backup/directory'&#13;
DATE=$(date +'%Y-%m-%d_%H-%M-%S')&#13;
&#13;
# 创建备份目录（如果不存在）&#13;
mkdir -p $BACKUP_DIR&#13;
&#13;
# 备份每个数据库&#13;
for DB_NAME in '${DATABASES[@]}'; do&#13;
  BACKUP_FILE='$BACKUP_DIR/${DB_NAME}_backup_$DATE.sql'&#13;
  mysqldump -h $DB_HOST -u $DB_USER -p$DB_PASS $DB_NAME &gt; $BACKUP_FILE&#13;
  &#13;
  # 检查备份是否成功&#13;
  if [ $? -eq 0 ]; then&#13;
    echo '[$DATE] Database backup successful: $BACKUP_FILE'&#13;
  else&#13;
    echo '[$DATE] Database backup failed for $DB_NAME' &gt;&amp;2&#13;
  fi&#13;
done&#13;
&#13;
# 保留最近7天的备份，删除更早的备份文件&#13;
find $BACKUP_DIR -type f -name '*_backup_*.sql' -mtime +7 -exec rm {} \;&#13;
&#13;
echo '[$DATE] Old backups cleaned up'&#13;
```&#13;
&#13;
备份整个数据库实例：&#13;
&#13;
```shell&#13;
#!/bin/bash&#13;
&#13;
# 配置部分&#13;
DB_HOST='localhost'&#13;
DB_USER='your_username'&#13;
DB_PASS='your_password'&#13;
BACKUP_DIR='/path/to/your/backup/directory'&#13;
DATE=$(date +'%Y-%m-%d_%H-%M-%S')&#13;
BACKUP_FILE='$BACKUP_DIR/all_databases_backup_$DATE.sql'&#13;
&#13;
# 创建备份目录（如果不存在）&#13;
mkdir -p $BACKUP_DIR&#13;
&#13;
# 备份整个 MySQL 实例&#13;
mysqldump -h $DB_HOST -u $DB_USER -p$DB_PASS --all-databases &gt; $BACKUP_FILE&#13;
&#13;
# 检查备份是否成功&#13;
if [ $? -eq 0 ]; then&#13;
  echo '[$DATE] All databases backup successful: $BACKUP_FILE'&#13;
else&#13;
  echo '[$DATE] All databases backup failed' &gt;&amp;2&#13;
  exit 1&#13;
fi&#13;
&#13;
# 保留最近7天的备份，删除更早的备份文件&#13;
find $BACKUP_DIR -type f -name 'all_databases_backup_*.sql' -mtime +7 -exec rm {} \;&#13;
&#13;
echo '[$DATE] Old backups cleaned up'&#13;
```&#13;
&#13;
### 恢复&#13;
&#13;
```shell&#13;
mysql -u your_username -p -e 'CREATE DATABASE IF NOT EXISTS database1;'&#13;
mysql -u your_username -p database1 &lt; /path/to/your/backup/directory/database1_backup.sql&#13;
# 恢复整个实例&#13;
mysql -u your_username -p &lt; /path/to/your/backup/directory/all_databases_backup.sql&#13;
```。</description><guid isPermaLink="true">https://blog.witter.top/post/MySQL-de-bei-fen---hui-fu.html</guid><pubDate>Fri, 05 Jul 2024 07:51:17 +0000</pubDate></item><item><title>有趣项目收集</title><link>https://blog.witter.top/post/you-qu-xiang-mu-shou-ji.html</link><description>&gt; [!NOTE]&#13;
&gt; 整理自Koala聊开源&#13;
&#13;
- [Cyber Scarecrow|赛博稻草人](https://www.cyberscarecrow.com)&#13;
- [Yazi|Rust 编写终端文件管理器](https://github.com/sxyazi/yazi)&#13;
- [Glasskube|K8s 包管理器](https://glasskube.dev/)。</description><guid isPermaLink="true">https://blog.witter.top/post/you-qu-xiang-mu-shou-ji.html</guid><pubDate>Fri, 05 Jul 2024 05:50:52 +0000</pubDate></item><item><title>Drone CI/CD</title><link>https://blog.witter.top/post/Drone%20CI-CD.html</link><description>[官方文档](https://docs.drone.io/)&#13;
&#13;
drone-server+ssh-runner &#13;
&#13;
docker compose 部署&#13;
&#13;
```yaml&#13;
services:&#13;
  drone:&#13;
    image: drone/drone:latest&#13;
    volumes:&#13;
      - ./:/data&#13;
    environment:&#13;
      - DRONE_GITHUB_CLIENT_ID=&#13;
      - DRONE_GITHUB_CLIENT_SECRET=&#13;
      - DRONE_RPC_SECRET=&#13;
      - DRONE_SERVER_HOST=&#13;
      - DRONE_SERVER_PROTO=http&#13;
    ports:&#13;
      - '8085:80'&#13;
      - '4433:443'&#13;
    restart: always&#13;
    container_name: drone&#13;
&#13;
  drone-runner:&#13;
    image: drone/drone-runner-ssh&#13;
    environment:&#13;
      - DRONE_RPC_PROTO=http&#13;
      - DRONE_RPC_HOST=&#13;
      - DRONE_RPC_SECRET=&#13;
    ports:&#13;
      - '3001:3000'&#13;
    restart: always&#13;
    container_name: runner-ssh&#13;
```&#13;
![image](https://github.com/ljwtorch/ljwtorch.github.io/assets/64066892/8aee94fa-7f40-48bc-be49-52f85decf879)&#13;
&#13;
。</description><guid isPermaLink="true">https://blog.witter.top/post/Drone%20CI-CD.html</guid><pubDate>Thu, 04 Jul 2024 07:17:58 +0000</pubDate></item><item><title>Kubernetes The Hard Way（译文）</title><link>https://blog.witter.top/post/Kubernetes%20The%20Hard%20Way%EF%BC%88-yi-wen-%EF%BC%89.html</link><description>&gt; [!IMPORTANT]&#13;
&gt; [GitHub原文链接](https://github.com/kelseyhightower/kubernetes-the-hard-way) 本文使用ChatGPT协助翻译&#13;
&#13;
# 先决条件&#13;
&#13;
在本实验中，您将回顾跟随本教程所需的机器要求。</description><guid isPermaLink="true">https://blog.witter.top/post/Kubernetes%20The%20Hard%20Way%EF%BC%88-yi-wen-%EF%BC%89.html</guid><pubDate>Sat, 29 Jun 2024 04:49:20 +0000</pubDate></item><item><title>Debian开启smb服务</title><link>https://blog.witter.top/post/Debian-kai-qi-smb-fu-wu.html</link><description>1. 安装samba：`sudo apt install samba smbclient cifs-utils`&#13;
&#13;
2. 创建samba目录&#13;
&#13;
   ```shell&#13;
   sudo mkdir /mnt/public&#13;
   sudo mkdir /mnt/private&#13;
   &#13;
   sudo vim /etc/samba/smb.conf&#13;
   &#13;
   [public]&#13;
      comment = Public Folder&#13;
      path = /mnt/public&#13;
      writable = yes&#13;
      guest ok = yes&#13;
      guest only = yes&#13;
      force create mode = 775&#13;
      force directory mode = 775&#13;
   [private]&#13;
      comment = Private Folder&#13;
      path = /mnt/private&#13;
      writable = yes&#13;
      guest ok = no&#13;
      valid users = @smbshare&#13;
      force create mode = 770&#13;
      force directory mode = 770&#13;
      inherit permissions = yes&#13;
   ```&#13;
&#13;
3. 创建共享用户和组&#13;
&#13;
   ```shell&#13;
   sudo groupadd smb&#13;
   sudo chgrp -R smb /mnt/private /mnt/public&#13;
   sudo chmod 2770 /mnt/private&#13;
   sudo chmod 2775 /mnt/public&#13;
   # 添加无需登录用户&#13;
   sudo useradd -M -s /sbin/nologin smbuser&#13;
   # 添加到共享组中&#13;
   sudo usermod -aG smb smbuser&#13;
   # 创建密码并启用&#13;
   sudo smbpasswd -a smbuser    smbpwd&#13;
   sudo smbpasswd -e smbuser&#13;
   ```&#13;
&#13;
4. 本地测试&#13;
&#13;
   ```shell&#13;
   sudo testparm&#13;
   ```&#13;
&#13;
5. 局域网连接：`smb://ip`&#13;
&#13;
。</description><guid isPermaLink="true">https://blog.witter.top/post/Debian-kai-qi-smb-fu-wu.html</guid><pubDate>Thu, 27 Jun 2024 06:28:34 +0000</pubDate></item><item><title>0-1 部署监控告警系统(Exporter)</title><link>https://blog.witter.top/post/0-1%20-bu-shu-jian-kong-gao-jing-xi-tong-%28Exporter%29.html</link><description>&gt; [!TIP]&#13;
&gt; 下方采集指标内容只写正在使用的一些指标名称，其他部分自行搭建后进行了解！&#13;
&#13;
## 1.node-exporter&#13;
&#13;
### 1.1 部署方式&#13;
&#13;
**单机部署**&#13;
&#13;
```shell&#13;
docker run -d --name node-exporter --restart=always --net='host' --pid='host' -v '/:/host:ro,rslave' quay.io/prometheus/node-exporter:latest --path.rootfs=/host&#13;
```&#13;
&#13;
**docker compose 部署**&#13;
&#13;
```yaml&#13;
  node-exporter:&#13;
    image: quay.io/prometheus/node-exporter:latest&#13;
    container_name: node-exporter&#13;
    restart: always&#13;
    network_mode: host&#13;
    pid: host&#13;
    volumes:&#13;
      - /:/host:ro,rslave&#13;
    command: --path.rootfs=/host&#13;
    depends_on:&#13;
      - prometheus&#13;
```&#13;
&#13;
访问 ip:9100&#13;
&#13;
### 1.2 采集指标&#13;
&#13;
1. CPU使用率：`round((1 - avg(rate(node_cpu_seconds_total{instance='instance:9100',mode='idle'}[1m])) by (instance)) * 100, 0.01)`&#13;
2. RAM使用率：`round((1 - (node_memory_MemAvailable_bytes{instance='instance:9100'} / (node_memory_MemTotal_bytes{instance='instance:9100'})))* 100,0.01)`&#13;
3. 磁盘使用率：`round((node_filesystem_size_bytes{instance='instance:9100',mountpoint='/'}-node_filesystem_free_bytes{instance='instance:9100',mountpoint='/'})*100/(node_filesystem_avail_bytes{instance='instance:9100',mountpoint='/'}+(node_filesystem_size_bytes{instance='instance:9100',mountpoint='/'}-node_filesystem_free_bytes{instance='instance:9100',mountpoint='/'})),0.01)`&#13;
4. 网络连接数使用量：`node_netstat_Tcp_CurrEstab{instance='instance:9100'}`&#13;
&#13;
&#13;
&#13;
## 2.blackbox-exporter&#13;
&#13;
### 2.1 部署方式&#13;
&#13;
**docker compose部署**&#13;
&#13;
```yaml&#13;
  blackbox-exporter:&#13;
    image: quay.io/prometheus/blackbox-exporter:latest&#13;
    container_name: blackbox-exporter&#13;
    restart: always&#13;
    ports:&#13;
      - '9115:9115'&#13;
    volumes:&#13;
      - ./blackbox-exporter/:/config&#13;
    command: --config.file=/config/blackbox.yml&#13;
    depends_on:&#13;
      - prometheus&#13;
```&#13;
&#13;
### 2.2 配置文件&#13;
&#13;
```yaml&#13;
modules:&#13;
  http_2xx:&#13;
    prober: http&#13;
  http_post_2xx:&#13;
    prober: http&#13;
    http:&#13;
      method: POST&#13;
  tcp_connect:&#13;
    prober: tcp&#13;
  pop3s_banner:&#13;
    prober: tcp&#13;
    tcp:&#13;
      query_response:&#13;
      - expect: '^+OK'&#13;
      tls: true&#13;
      tls_config:&#13;
        insecure_skip_verify: false&#13;
  ssh_banner:&#13;
    prober: tcp&#13;
    tcp:&#13;
      query_response:&#13;
      - expect: '^SSH-2.0-'&#13;
  irc_banner:&#13;
    prober: tcp&#13;
    tcp:&#13;
      query_response:&#13;
      - send: 'NICK prober'&#13;
      - send: 'USER prober prober prober :prober'&#13;
      - expect: 'PING :([^ ]+)'&#13;
        send: 'PONG ${1}'&#13;
      - expect: '^:[^ ]+ 001'&#13;
  icmp:&#13;
    prober: icmp &#13;
  http_2xx_java: # 自定义状态码200的匹配规则&#13;
    prober: http&#13;
    http:&#13;
      method: GET&#13;
      fail_if_body_not_matches_regexp:&#13;
      - ''status':'UP''&#13;
```&#13;
&#13;
### 2.3 采集指标&#13;
&#13;
1. 状态码：`probe_http_status_code`&#13;
2. ping延时：`probe_icmp_duration_seconds`&#13;
3. tcp连接：`probe_success`&#13;
&#13;
## 3.jmx-exporter&#13;
&#13;
&gt; 目前只用来监控kafka实例&#13;
&#13;
### 3.1 部署方式&#13;
&#13;
**本地部署**  [[下载链接](https://github.com/prometheus/jmx_exporter/tree/release-0.20.0)](https://github.com/prometheus/jmx_exporter/tree/release-0.20.0)&#13;
&#13;
监控kafka批量启动脚本：`bash jmx_kafka.sh status`&#13;
&#13;
```shell&#13;
#!/bin/bash&#13;
&#13;
# 定义一个函数来处理不同的 jmx 实例&#13;
manage_jmx_instance() {&#13;
  local instance='$1'&#13;
  local port=$((9800 + ${instance#kafka})) # 提取实例名称后的数字并加9800作为端口&#13;
  local pid_file='/var/run/jmx_${instance}_httpserver.pid'&#13;
  local log_file='/root/monitor/log/${instance}.log'&#13;
&#13;
  case '$2' in&#13;
    start)&#13;
      if [ -f '$pid_file' ]; then&#13;
        echo 'jmx_${instance}_httpserver is already running.'&#13;
      else&#13;
        # 使用对应的端口号启动 jmx_prometheus_httpserver&#13;
        nohup java -jar /root/monitor/jmx_prometheus_httpserver-0.20.0.jar $port /root/monitor/${instance}.yml &gt;&gt; '$log_file' 2&gt;&amp;1 &amp;&#13;
        echo $! &gt; '$pid_file'&#13;
        echo 'jmx_${instance}_httpserver on port $port started.'&#13;
      fi&#13;
      ;;&#13;
    status)&#13;
      if [ -f '$pid_file' ]; then&#13;
        PID=$(cat '$pid_file')&#13;
        ps -p $PID &gt; /dev/null 2&gt;&amp;1&#13;
        if [ $? -eq 0 ]; then&#13;
          echo 'jmx_${instance}_httpserver is running (PID $PID).'&#13;
        else&#13;
          echo 'jmx_${instance}_httpserver is not running but PID file exists.'&#13;
        fi&#13;
      else&#13;
        echo 'jmx_${instance}_httpserver is not running.'&#13;
      fi&#13;
      ;;&#13;
    stop)&#13;
      if [ -f '$pid_file' ]; then&#13;
        PID=$(cat '$pid_file')&#13;
        # 使用 kill 而不是 kill -9 以避免过于激进地终止进程&#13;
        kill $PID&#13;
        rm -f '$pid_file'&#13;
        echo 'jmx_${instance}_httpserver stopped.'&#13;
      else&#13;
        echo 'jmx_${instance}_httpserver is not running.'&#13;
      fi&#13;
      ;;&#13;
    *)&#13;
      echo 'Usage: $0 {start|status|stop}'&#13;
      exit 1&#13;
      ;;&#13;
  esac&#13;
}&#13;
&#13;
# 根据提供的选项调用函数来管理不同的 jmx 实例,同目录下存放了三个kafka1-3.yml配置文件供exporter使用&#13;
for instance in kafka1 kafka2 kafka3; do&#13;
  manage_jmx_instance '$instance' '$1'&#13;
done&#13;
&#13;
exit 0&#13;
```&#13;
&#13;
### 3.2 配置文件&#13;
&#13;
`kafka1.yml`&#13;
&#13;
```yaml&#13;
lowercaseOutputName: true&#13;
hostPort: kafka_ip:9999&#13;
rules:&#13;
# Special cases and very specific rules&#13;
- pattern : kafka.server&lt;type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)&gt;&lt;&gt;Value&#13;
  name: kafka_server_$1_$2&#13;
  type: GAUGE&#13;
  labels:&#13;
    clientId: '$3'&#13;
    topic: '$4'&#13;
    partition: '$5'&#13;
- pattern : kafka.server&lt;type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)&gt;&lt;&gt;Value&#13;
  name: kafka_server_$1_$2&#13;
  type: GAUGE&#13;
  labels:&#13;
    clientId: '$3'&#13;
    broker: '$4:$5'&#13;
- pattern : kafka.coordinator.(\w+)&lt;type=(.+), name=(.+)&gt;&lt;&gt;Value&#13;
  name: kafka_coordinator_$1_$2_$3&#13;
  type: GAUGE&#13;
&#13;
# Generic per-second counters with 0-2 key/value pairs&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)&gt;&lt;&gt;Count&#13;
  name: kafka_$1_$2_$3_total&#13;
  type: COUNTER&#13;
  labels:&#13;
    '$4': '$5'&#13;
    '$6': '$7'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)PerSec\w*, (.+)=(.+)&gt;&lt;&gt;Count&#13;
  name: kafka_$1_$2_$3_total&#13;
  type: COUNTER&#13;
  labels:&#13;
    '$4': '$5'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)PerSec\w*&gt;&lt;&gt;Count&#13;
  name: kafka_$1_$2_$3_total&#13;
  type: COUNTER&#13;
&#13;
# Quota specific rules&#13;
- pattern: kafka.server&lt;type=(.+), user=(.+), client-id=(.+)&gt;&lt;&gt;([a-z-]+)&#13;
  name: kafka_server_quota_$4&#13;
  type: GAUGE&#13;
  labels:&#13;
    resource: '$1'&#13;
    user: '$2'&#13;
    clientId: '$3'&#13;
- pattern: kafka.server&lt;type=(.+), client-id=(.+)&gt;&lt;&gt;([a-z-]+)&#13;
  name: kafka_server_quota_$3&#13;
  type: GAUGE&#13;
  labels:&#13;
    resource: '$1'&#13;
    clientId: '$2'&#13;
- pattern: kafka.server&lt;type=(.+), user=(.+)&gt;&lt;&gt;([a-z-]+)&#13;
  name: kafka_server_quota_$3&#13;
  type: GAUGE&#13;
  labels:&#13;
    resource: '$1'&#13;
    user: '$2'&#13;
&#13;
# Generic gauges with 0-2 key/value pairs&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)&gt;&lt;&gt;Value&#13;
  name: kafka_$1_$2_$3&#13;
  type: GAUGE&#13;
  labels:&#13;
    '$4': '$5'&#13;
    '$6': '$7'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+), (.+)=(.+)&gt;&lt;&gt;Value&#13;
  name: kafka_$1_$2_$3&#13;
  type: GAUGE&#13;
  labels:&#13;
    '$4': '$5'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)&gt;&lt;&gt;Value&#13;
  name: kafka_$1_$2_$3&#13;
  type: GAUGE&#13;
&#13;
# Emulate Prometheus 'Summary' metrics for the exported 'Histogram's.&#13;
#&#13;
# Note that these are missing the '_sum' metric!&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)&gt;&lt;&gt;Count&#13;
  name: kafka_$1_$2_$3_count&#13;
  type: COUNTER&#13;
  labels:&#13;
    '$4': '$5'&#13;
    '$6': '$7'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)&gt;&lt;&gt;(\d+)thPercentile&#13;
  name: kafka_$1_$2_$3&#13;
  type: GAUGE&#13;
  labels:&#13;
    '$4': '$5'&#13;
    '$6': '$7'&#13;
    quantile: '0.$8'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+), (.+)=(.+)&gt;&lt;&gt;Count&#13;
  name: kafka_$1_$2_$3_count&#13;
  type: COUNTER&#13;
  labels:&#13;
    '$4': '$5'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+), (.+)=(.*)&gt;&lt;&gt;(\d+)thPercentile&#13;
  name: kafka_$1_$2_$3&#13;
  type: GAUGE&#13;
  labels:&#13;
    '$4': '$5'&#13;
    quantile: '0.$6'&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)&gt;&lt;&gt;Count&#13;
  name: kafka_$1_$2_$3_count&#13;
  type: COUNTER&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)&gt;&lt;&gt;(\d+)thPercentile&#13;
  name: kafka_$1_$2_$3&#13;
  type: GAUGE&#13;
  labels:&#13;
    quantile: '0.$4'&#13;
&#13;
# Generic gauges for MeanRate Percent&#13;
# Ex) kafka.server&lt;type=KafkaRequestHandlerPool, name=RequestHandlerAvgIdlePercent&gt;&lt;&gt;MeanRate&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)Percent\w*&gt;&lt;&gt;MeanRate&#13;
  name: kafka_$1_$2_$3_percent&#13;
  type: GAUGE&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)Percent\w*&gt;&lt;&gt;Value&#13;
  name: kafka_$1_$2_$3_percent&#13;
  type: GAUGE&#13;
- pattern: kafka.(\w+)&lt;type=(.+), name=(.+)Percent\w*, (.+)=(.+)&gt;&lt;&gt;Value&#13;
  name: kafka_$1_$2_$3_percent&#13;
  type: GAUGE&#13;
  labels:&#13;
    '$4': '$5'&#13;
```&#13;
&#13;
### 3.3 采集指标&#13;
&#13;
1. 指定kafka-topic消息数量：`round(increase(kafka_server_brokertopicmetrics_messagesin_total{topic='kafka_topic'}[1h]),0.01)`&#13;
&#13;
## 4.mysql-exporter&#13;
&#13;
### 4.1 部署方式&#13;
&#13;
```shell&#13;
docker run -d --name mysql-exporter --restart always -p 9104:9104 -e DATA_SOURCE_NAME='user:passwd@(ip:3306)/' prom/mysqld-exporter&#13;
```&#13;
&#13;
### 4.2 指标名称&#13;
&#13;
1. Mysql监控agent存活：mysql_up&#13;
2. 连接数：mysql_global_status_threads_connected&#13;
3. 文件打开数：mysql_global_status_innodb_num_open_files&#13;
4. 从库只读：mysql_global_variables_read_only&#13;
5. 主从延迟：mysql_slave_status_seconds_behind_master&#13;
6. sql线程：mysql_slave_status_slave_sql_running&#13;
7. IO线程：mysql_slave_status_slave_io_running&#13;
8. 入口流量：mysql_global_status_bytes_received&#13;
9. 出口流量：mysql_global_status_bytes_sent&#13;
10. 写操作速率：mysql_global_status_commands_total&#13;
11. 慢查询：mysql_global_status_slow_queries&#13;
12. 查询速率：mysql_global_status_questions&#13;
13. 可用连接数：mysql_global_variables_max_connections&#13;
14. 缓冲池利用率：mysql_global_status_buffer_pool_pages&#13;
15. 打开文件数量限制：mysql_global_variables_open_files_limit&#13;
16. 启用了InnoDB强制恢复功能：mysql_global_variables_innodb_force_recovery&#13;
17. InnoDB日志文件大小：mysql_global_variables_innodb_log_file_size&#13;
18. InnoDB插件已启用：mysql_global_variables_ignore_builtin_innodb&#13;
19. Binary Log状态：mysql_global_variables_log_bin&#13;
20. Binlog缓存大小：mysql_global_variables_binlog_cache_size&#13;
21. 同步Binlog状态：mysql_global_variables_sync_binlog&#13;
22. IO线程状态：mysql_slave_status_slave_io_running&#13;
23. sql线程状态：mysql_slave_status_slave_sql_running&#13;
24. InnoDB日志等待量：mysql_global_status_innodb_log_waits （不为0的话，InnoDB log buffer因空间不足而等待）。</description><guid isPermaLink="true">https://blog.witter.top/post/0-1%20-bu-shu-jian-kong-gao-jing-xi-tong-%28Exporter%29.html</guid><pubDate>Mon, 24 Jun 2024 06:33:28 +0000</pubDate></item><item><title>0-1 部署监控告警系统(Prometheus)</title><link>https://blog.witter.top/post/0-1%20-bu-shu-jian-kong-gao-jing-xi-tong-%28Prometheus%29.html</link><description>**完整docker-compose.yml**&#13;
&#13;
```yaml&#13;
networks:&#13;
  monitor:&#13;
    name: monitor&#13;
    driver: bridge&#13;
&#13;
services:&#13;
  prometheus:&#13;
    image: prom/prometheus:latest&#13;
    ports:&#13;
      - 9090:9090&#13;
    restart: always&#13;
    volumes:&#13;
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml&#13;
      - ./prometheus/:/etc/prometheus&#13;
    networks:&#13;
      - monitor&#13;
    container_name: prometheus&#13;
    environment:&#13;
      - --web.enable-admin-api&#13;
      - --web.enable-lifecycle&#13;
&#13;
  node-exporter:&#13;
    image: quay.io/prometheus/node-exporter:latest&#13;
    container_name: node-exporter&#13;
    restart: always&#13;
    network_mode: host&#13;
    pid: host&#13;
    volumes:&#13;
      - /:/host:ro,rslave&#13;
    command: --path.rootfs=/host&#13;
    depends_on:&#13;
      - prometheus&#13;
&#13;
  blackbox-exporter:&#13;
    image: quay.io/prometheus/blackbox-exporter:latest&#13;
    container_name: blackbox-exporter&#13;
    restart: always&#13;
    ports:&#13;
      - '9115:9115'&#13;
    volumes:&#13;
      - ./blackbox-exporter/:/config&#13;
    command: --config.file=/config/blackbox.yml&#13;
    depends_on:&#13;
      - prometheus&#13;
&#13;
  alertmanager:&#13;
      image: prom/alertmanager:latest&#13;
      container_name: alertmanager&#13;
      restart: always&#13;
      volumes:&#13;
          - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml&#13;
      ports:&#13;
          - '9093:9093'&#13;
      networks:&#13;
          - monitor&#13;
&#13;
  dingtalk-webhook:&#13;
    image: timonwong/prometheus-webhook-dingtalk:latest&#13;
    restart: always&#13;
    volumes:&#13;
      - ./dingtalk-webhook/config.yml:/etc/prometheus-webhook-dingtalk/config.yml&#13;
      - ./dingtalk-webhook/prometheus-webhook-dingtalk:/prometheus-webhook-dingtalk&#13;
      - ./dingtalk-webhook/templates:/etc/prometheus-webhook-dingtalk/templates&#13;
    ports:&#13;
      - 8060:8060&#13;
    command: '--web.enable-ui'&#13;
    networks:&#13;
      - monitor&#13;
    container_name: dingtalk-webhook&#13;
    depends_on:&#13;
      - prometheus&#13;
      - node-exporter&#13;
      - blackbox-exporter&#13;
&#13;
  grafana:&#13;
    image: grafana/grafana-enterprise&#13;
    user: root&#13;
    restart: always&#13;
    ports:&#13;
      - 3000:3000&#13;
    networks:&#13;
      - monitor&#13;
    volumes:&#13;
      - ./grafana/data:/var/lib/grafana&#13;
    container_name: grafana&#13;
    depends_on:&#13;
      - prometheus&#13;
```&#13;
&#13;
## 1.Prometheus&#13;
&#13;
&gt; Prometheus只说明进行自我修改的部分，基本的安装步骤不再赘述！&#13;
&#13;
### 1.1 部署方式&#13;
&#13;
**本机部署**&#13;
&#13;
二进制文件[[下载链接](https://prometheus.io/download/)](https://prometheus.io/download/)&#13;
&#13;
```shell&#13;
# prometheus-2.53.0.linux-amd64.tar.gz 对下载后的二进制文件进行解压&#13;
&#13;
# 首次启动命令&#13;
nohup ./prometheus --config.file=prometheus.yml --storage.tsdb.retention.time=90d --web.enable-lifecycle &amp;&#13;
&#13;
# check.sh -&gt; 二进制文件部署方式的热重载脚本&#13;
#========================================&#13;
#!/bin/bash&#13;
# 定义Prometheus配置文件路径&#13;
prometheus_config='./prometheus/prometheus.yml'&#13;
# 检查配置文件&#13;
check_result=$(./promtool check config '$prometheus_config' 2&gt;&amp;1)&#13;
if [[ $check_result =~ 'FAILED' ]]; then&#13;
  # 配置文件检查失败，输出错误信息&#13;
  echo 'Prometheus config check failed!'&#13;
  echo '$check_result'&#13;
  exit 1&#13;
else&#13;
  # 配置文件检查成功，执行重载&#13;
  curl -X POST http://localhost:9090/-/reload&#13;
  echo 'Prometheus config check successful!'&#13;
fi&#13;
#========================================&#13;
```&#13;
&#13;
**systemd部署**&#13;
&#13;
```shell&#13;
# 可选添加用户&#13;
sudo useradd --no-create-home --shell /bin/false prometheus&#13;
# prometheus.service 文件&#13;
&#13;
[Unit]&#13;
Description=Prometheus&#13;
Wants=network-online.target&#13;
After=network-online.target&#13;
&#13;
[Service]&#13;
User=prometheus&#13;
Group=prometheus&#13;
Type=simple&#13;
ExecStart=/usr/local/bin/prometheus \&#13;
  --config.file=/etc/prometheus/prometheus.yml \&#13;
  --storage.tsdb.path=/var/lib/prometheus/ \&#13;
  --web.console.templates=/usr/share/prometheus/consoles \&#13;
  --web.console.libraries=/usr/share/prometheus/console_libraries&#13;
&#13;
[Install]&#13;
WantedBy=multi-user.target&#13;
&#13;
# 进行对应的初次重载命令&#13;
sudo systemctl daemon-reload&#13;
sudo systemctl enable prometheus&#13;
sudo systemctl start prometheus&#13;
sudo systemctl status prometheus&#13;
```&#13;
&#13;
**docker-compose部署**&#13;
&#13;
```yaml&#13;
networks:&#13;
  monitor:&#13;
    name: monitor&#13;
    driver: bridge&#13;
&#13;
services:&#13;
  prometheus:&#13;
    image: prom/prometheus:latest&#13;
    ports:&#13;
      - 9090:9090&#13;
    restart: always&#13;
    volumes:&#13;
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml&#13;
      - ./prometheus/:/etc/prometheus&#13;
    networks:&#13;
      - monitor&#13;
    container_name: prometheus&#13;
    environment:&#13;
      - --web.enable-admin-api&#13;
      - --web.enable-lifecycle&#13;
```&#13;
&#13;
### 1.2 主配置文件&#13;
&#13;
&gt; 采用Git+Jenkins(CI/CD)+Docker方式，通过本地编辑 -&gt; 推送Git -&gt; CI/CD执行拉取和热更新 -&gt; 完成配置文件修改步骤&#13;
&#13;
`prometheus.yml`&#13;
&#13;
```yaml&#13;
# my global config&#13;
global:&#13;
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.&#13;
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.&#13;
  # scrape_timeout is set to the global default (10s).&#13;
&#13;
# Alertmanager configuration&#13;
alerting:&#13;
  alertmanagers:&#13;
    - static_configs:&#13;
        - targets:&#13;
           - alertmanager:9093&#13;
&#13;
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.&#13;
# 可以为通配符路径，注意是宿主机路径还是容器路径&#13;
rule_files:&#13;
  # - 'first_rules.yml'&#13;
  # - 'second_rules.yml'&#13;
  - ./rules/*.yml&#13;
&#13;
# A scrape configuration containing exactly one endpoint to scrape:&#13;
# Here it's Prometheus itself.&#13;
scrape_configs:&#13;
  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.&#13;
  - job_name: 'prometheus'&#13;
    # metrics_path defaults to '/metrics'&#13;
    # scheme defaults to 'http'.&#13;
    static_configs:&#13;
      - targets: ['localhost:9090']&#13;
&#13;
  - job_name: 'node_exporter'&#13;
    metrics_path: /metrics&#13;
    static_configs:&#13;
    file_sd_configs:&#13;
      - files: ['./node-exporter/*.yml']&#13;
        refresh_interval: 15s&#13;
    relabel_configs:&#13;
      - source_labels: [__address__]&#13;
        regex: (.*)&#13;
        target_label: __address__&#13;
        replacement: $1&#13;
&#13;
  - job_name: 'blackbox_http_2xx'&#13;
    scrape_interval: 90s&#13;
    metrics_path: /probe&#13;
    params:&#13;
      module:&#13;
      - http_2xx&#13;
    static_configs:&#13;
    file_sd_configs:&#13;
      - files: ['./blackbox_http_2xx/*.yml']&#13;
        refresh_interval: 15s&#13;
    relabel_configs:&#13;
      - source_labels: [__address__]&#13;
        target_label: __param_target&#13;
      - source_labels: [__param_target]&#13;
        target_label: instance&#13;
      - target_label: __address__&#13;
        replacement: localhost:9115  # blackbox_exporter所在的机器和端口&#13;
&#13;
  - job_name: 'blackbox_http_3min'&#13;
    scrape_interval: 3m # 可以单独调整blackbox的抓取频率&#13;
    metrics_path: /probe&#13;
    params:&#13;
      module:&#13;
      - http_2xx&#13;
    static_configs:&#13;
    file_sd_configs:&#13;
      - files: ['./blackbox_http_3min/*.yml']&#13;
        refresh_interval: 15s&#13;
    relabel_configs:&#13;
      - source_labels: [__address__]&#13;
        target_label: __param_target&#13;
      - source_labels: [__param_target]&#13;
        target_label: instance&#13;
      - target_label: __address__&#13;
        replacement: localhost:9115  # blackbox_exporter所在的机器和端口&#13;
&#13;
  - job_name: 'blackbox_tcp_connect'&#13;
    metrics_path: /probe&#13;
    params:&#13;
      module:&#13;
      - tcp_connect&#13;
    static_configs:&#13;
    file_sd_configs:&#13;
      - files: ['./blackbox_tcp_connect/*.yml']&#13;
        refresh_interval: 15s&#13;
    relabel_configs:&#13;
      - source_labels: [__address__]&#13;
        target_label: __param_target&#13;
      - source_labels: [__param_target]&#13;
        target_label: instance&#13;
      - target_label: __address__&#13;
        replacement: localhost:9115  # blackbox_exporter所在的机器和端口&#13;
&#13;
  - job_name: 'blackbox_ping'&#13;
    metrics_path: /probe&#13;
    params:&#13;
      module:&#13;
      - icmp&#13;
    static_configs:&#13;
    file_sd_configs:&#13;
      - files: ['./blackbox_ping/*.yml']&#13;
        refresh_interval: 15s&#13;
    relabel_configs:&#13;
      - source_labels: [__address__]&#13;
        target_label: __param_target&#13;
      - source_labels: [__param_target]&#13;
        target_label: instance&#13;
      - target_label: __address__&#13;
        replacement: localhost:9115  # blackbox_exporter所在的机器和端口&#13;
&#13;
# 以下为大数据Hadoop所需的exporter&#13;
  - job_name: 'fsimage'&#13;
    scrape_interval: 1m   # Depends on how often the name node writes a fsimage file.&#13;
    scrape_timeout: 40s    # Depends on size&#13;
    static_configs:&#13;
      - targets: [ 'xxxx:9709' ]&#13;
        labels:&#13;
          project: fsimage&#13;
&#13;
  - job_name: 'kafka-exporter'&#13;
    scrape_interval: 20s&#13;
    scrape_timeout: 15s&#13;
    static_configs:&#13;
      - targets: ['xxxx:9308']&#13;
        labels:&#13;
          project: kafka-exporter&#13;
          instance: xxxx:9308&#13;
&#13;
  - job_name: 'jmx-exporter'&#13;
    metrics_path: /metrics&#13;
    static_configs:&#13;
    file_sd_configs:&#13;
      - files: ['./jmx-exporter/*.yml']&#13;
        refresh_interval: 15s&#13;
```&#13;
&#13;
### 1.3 从配置文件&#13;
&#13;
`node-exporter.yml`&#13;
&#13;
```yaml&#13;
- targets:&#13;
  - xxxx:9100&#13;
  labels:    &#13;
    project: configure by ur self&#13;
    instance: xxx:9100&#13;
    name: configure by ur self&#13;
```&#13;
&#13;
`blackbox-exporter.yml`&#13;
&#13;
```yaml&#13;
- targets:&#13;
  - put a url on this line, include https or http&#13;
  labels:&#13;
    project: configure by ur self&#13;
    service: xxx&#13;
```&#13;
&#13;
`jmx-exporter.yml`&#13;
&#13;
```yaml&#13;
- targets:&#13;
    - kafka:9803&#13;
  labels:&#13;
    project: node-exporter&#13;
    instance: kafka:9803&#13;
    process: kafka&#13;
```&#13;
&#13;
`node-rules.yml`&#13;
&#13;
```yaml&#13;
groups:&#13;
  - name: 服务器状态监控&#13;
    rules:&#13;
      - alert: CPU 监控&#13;
        expr: round((1 - avg(rate(node_cpu_seconds_total{instance='xxxxx:9100',mode='idle'}[1m])) by (instance)) * 100, 0.01) &gt; 80&#13;
        for: 1m&#13;
        labels:&#13;
          severity: critical&#13;
        annotations:&#13;
          summary: 服务器CPU占用率高&#13;
          description: '{{$labels.instance}}服务器 CPU 已使用: {{$value}}%'&#13;
      - alert: 内存监控&#13;
        expr: round((1 - (node_memory_MemAvailable_bytes{instance='xxxxx:9100'} / (node_memory_MemTotal_bytes{instance='xxxxx:9100'})))* 100,0.01) &gt; 95&#13;
        for: 1m&#13;
        labels:&#13;
          severity: critical&#13;
        annotations:&#13;
          summary: 服务器内存占用过高&#13;
          description: '{{$labels.instance}}服务器内存已使用: {{$value}}%'&#13;
      - alert: 磁盘监控&#13;
        expr: round((node_filesystem_size_bytes{instance='xxxxx:9100',mountpoint='/'}-node_filesystem_free_bytes{instance='xxxxx:9100',mountpoint='/'})*100/(node_filesystem_avail_bytes{instance='xxxxx:9100',mountpoint='/'}+(node_filesystem_size_bytes{instance='xxxxx:9100',mountpoint='/'}-node_filesystem_free_bytes{instance='xxxxx:9100',mountpoint='/'})),0.01) &gt; 90&#13;
        for: 1m&#13;
        labels:&#13;
          severity: critical&#13;
        annotations:&#13;
          summary: 服务器磁盘占用过高&#13;
          description: '{{$labels.instance}}服务器磁盘已使用: {{$value}}%'&#13;
      - alert: 网络连接数监控&#13;
        expr: node_netstat_Tcp_CurrEstab{instance='xxxxx:9100'} &gt; 500&#13;
        for: 1m&#13;
        labels:&#13;
          severity: critical&#13;
        annotations:&#13;
          summary: 服务器网络连接数过高&#13;
          description: '{{$labels.instance}}服务器当前网络连接数: {{$value}}'&#13;
```&#13;
&#13;
`blackbox-rules.yml`&#13;
&#13;
```yaml&#13;
groups:&#13;
- name: hosts监控&#13;
  rules:&#13;
  - alert: 状态码监控(monitor)&#13;
    expr: probe_http_status_code{service='xxx'} != 200&#13;
    for: 1m&#13;
    labels:&#13;
      severity: critical&#13;
    annotations:&#13;
      summary: xxx状态码不为200(monitor)&#13;
      description: '检测服务: {{$labels.service}}, 状态码为: {{$value}}'&#13;
```&#13;
&#13;
## 2.Grafana&#13;
&#13;
&gt; Grafana不在赘述配置dashboard等基础环节，只写一些流传较为少的配置方法&#13;
&#13;
### 2.1 部署方式&#13;
&#13;
**docker-compose部署**&#13;
&#13;
*在执行docker compose up -d之前，需要先执行docker run的步骤将想要持久化的容器内文件通过docker cp拷贝到宿主机上！*&#13;
&#13;
```yaml&#13;
  grafana:&#13;
    image: grafana/grafana-enterprise&#13;
    user: root&#13;
    restart: always&#13;
    ports:&#13;
      - 3000:3000&#13;
    networks:&#13;
      - monitor&#13;
    volumes:&#13;
      - ./grafana/data:/var/lib/grafana&#13;
      - ./grafana/plugins:/var/lib/grafana/plugins&#13;
      - ./grafana/log:/var/log/grafana&#13;
      - ./grafana/conf:/etc/grafana&#13;
    container_name: grafana&#13;
    depends_on:&#13;
      - prometheus&#13;
    extra_hosts:&#13;
      - 'url:ip_address'&#13;
```&#13;
&#13;
### 2.2 连接LDAP&#13;
&#13;
**设置配置文件中的auth.ldap为true  文件路径：`/usr/share/grafana/conf/defaults.ini`**&#13;
&#13;
```ini&#13;
[auth.ldap]&#13;
# Set to `true` to enable LDAP integration (default: `false`)&#13;
enabled = true&#13;
&#13;
# Path to the LDAP specific configuration file (default: `/etc/grafana/ldap.toml`)&#13;
config_file = /etc/grafana/ldap.toml&#13;
&#13;
# Allow sign-up should be `true` (default) to allow Grafana to create users on successful LDAP authentication.&#13;
# If set to `false` only already existing Grafana users will be able to login.&#13;
allow_sign_up = true&#13;
```&#13;
&#13;
**添加ldap相关的设置   文件路径：`/etc/grafana/ldap.toml`**&#13;
&#13;
```toml&#13;
# To troubleshoot and get more log info enable ldap debug logging in grafana.ini&#13;
# [log]&#13;
# filters = ldap:debug&#13;
[[servers]]&#13;
# Ldap server host (specify multiple hosts space separated)&#13;
host = 'xxxxx'&#13;
# Default port is 389 or 636 if use_ssl = true&#13;
port = 636&#13;
# Set to true if LDAP server should use an encrypted TLS connection (either with STARTTLS or LDAPS)&#13;
use_ssl = true&#13;
# If set to true, use LDAP with STARTTLS instead of LDAPS&#13;
start_tls = false&#13;
# set to true if you want to skip ssl cert validation&#13;
ssl_skip_verify = false&#13;
# set to the path to your root CA certificate or leave unset to use system defaults&#13;
# root_ca_cert = '/path/to/certificate.crt'&#13;
# Authentication against LDAP servers requiring client certificates&#13;
# client_cert = '/path/to/client.crt'&#13;
# client_key = '/path/to/client.key'&#13;
&#13;
# Search user bind dn&#13;
bind_dn = 'uid=xxxxx,cn=users,cn=accounts,dc=xxxxxxx,dc=cn'&#13;
# Search user bind password&#13;
# If the password contains # or ; you have to wrap it with triple quotes. Ex '''#password;'''&#13;
bind_password = ''''''&#13;
&#13;
# Timeout in seconds (applies to each host specified in the 'host' entry (space separated))&#13;
timeout = 10&#13;
&#13;
# User search filter, for example '(cn=%s)' or '(sAMAccountName=%s)' or '(uid=%s)'&#13;
search_filter = '(uid=%s)'&#13;
&#13;
# An array of base dns to search through&#13;
search_base_dns = ['cn=users,cn=accounts,dc=xxx,dc=cn']&#13;
&#13;
## For Posix or LDAP setups that does not support member_of attribute you can define the below settings&#13;
## Please check grafana LDAP docs for examples&#13;
#group_search_filter = '(&amp;(objectClass=groupOfNames)(memberOf=%s))'&#13;
#group_search_base_dns = ['cn=groups,cn=accounts,dc=xxx,dc=cn']&#13;
#group_search_filter_user_attribute = 'uid'&#13;
&#13;
# Specify names of the ldap attributes your ldap uses&#13;
[servers.attributes]&#13;
name = 'givenName'&#13;
surname = 'uid'&#13;
username = 'uid'&#13;
member_of = 'memberOf'&#13;
email =  'mail'&#13;
&#13;
# Map ldap groups to grafana org roles&#13;
[[servers.group_mappings]]&#13;
group_dn = 'cn=admins,cn=groups,cn=accounts,dc=xxxxx,dc=cn'&#13;
org_role = 'Admin'&#13;
# To make user an instance admin  (Grafana Admin) uncomment line below&#13;
grafana_admin = true&#13;
# The Grafana organization database id, optional, if left out the default org (id 1) will be used&#13;
# org_id = 1&#13;
&#13;
[[servers.group_mappings]]&#13;
group_dn = 'cn=users,cn=groups,cn=accounts,dc=xxxxx,dc=cn'&#13;
org_role = 'Editor'&#13;
&#13;
#[[servers.group_mappings]]&#13;
# If you want to match all (or no ldap groups) then you can use wildcard&#13;
#group_dn = '*'&#13;
#org_role = 'Viewer'&#13;
```。</description><guid isPermaLink="true">https://blog.witter.top/post/0-1%20-bu-shu-jian-kong-gao-jing-xi-tong-%28Prometheus%29.html</guid><pubDate>Sun, 23 Jun 2024 14:32:24 +0000</pubDate></item><item><title>Jenkins-Kubernetes相结合</title><link>https://blog.witter.top/post/Jenkins-Kubernetes-xiang-jie-he.html</link><description>&#13;
## 1.安装/部署&#13;
&#13;
### 1.1 docker compose&#13;
&#13;
`docker-compose.yml`&#13;
&#13;
```yaml&#13;
services:                                     &#13;
  jenkins:&#13;
    restart: always                            &#13;
    image: jenkins/jenkins:2.414.3  &#13;
    #image: jenkins/jenkins:2.387.2 &#13;
    container_name: jenkins&#13;
    network_mode: host&#13;
    ports:                                     &#13;
      - 18080:8080                              &#13;
      - 28888:28888&#13;
    volumes:&#13;
      - ./:/var/jenkins_home  &#13;
      - /usr/bin/docker:/usr/bin/docker               &#13;
      - /usr/local/bin/docker-compose:/usr/local/bin/docker-compose&#13;
      - /var/run/docker.sock:/var/run/docker.sock&#13;
    environment:&#13;
      - TZ=Asia/Shanghai&#13;
      - JENKINS_SLAVE_AGENT_PORT=28888&#13;
```&#13;
&#13;
## 2.Pipeline(流水线任务)&#13;
&#13;
### 2.1 Jenkinsfile概览&#13;
&#13;
*测试环境将服务部署到k8s中的Jenkinsfile，包括了项目代码拉取、配置文件拉取、项目编译、docker镜像构建/上传、多agent-workspace操作、Pod首次发布和滚动更新检测*&#13;
&#13;
```groovy&#13;
pipeline {&#13;
    agent {&#13;
        label 'node_X'&#13;
    }&#13;
    tools {&#13;
        jdk 'jdk11'&#13;
    }&#13;
    environment {&#13;
        //一般只需要修改这里几个&#13;
        CONTAINER_NAME = 'xxx'	//容器tag&#13;
        YAML_NAME = 'xxx-deploy.yml'	//yml文件名&#13;
        PUB_ENV = 'test'	//发布环境，对应以后进行版本控制的目录位置&#13;
        K8S_NAMESPACE = 'test-xxx'&#13;
        PROJECT_GIT_URL = ''       //项目代码地址&#13;
        CONFIG_GIT_URL = ''        //配置文件地址(当项目Jenkinsfile和项目不在同一仓库，这里指存放所有Jenkinsfile的仓库)&#13;
&#13;
        DOCKER_REGISTER_CREDS = credentials('registry_xxx')	//docker registry凭证&#13;
        DOCKER_REGISTRY = 'xxxxx'	//Docker仓库地址&#13;
        DOCKER_NAMESPACE = 'xxxxx'	//命名空间&#13;
        DOCKER_IMAGE = '${DOCKER_REGISTRY}/${DOCKER_NAMESPACE}/${CONTAINER_NAME}'	//组合成完整的镜像名称&#13;
    }&#13;
    stages {&#13;
        stage('get timestamp') {&#13;
            steps {&#13;
                script {&#13;
                    //获取当前时间戳&#13;
                    def timestamp = sh(returnStdout: true, script: 'date +%Y%m%d%H%M%S').trim()&#13;
                    //时间戳赋值&#13;
                    env.TIMESTAMP = timestamp&#13;
                }&#13;
            }&#13;
        }&#13;
        stage('fetch code') {&#13;
            steps {&#13;
                echo '----------Fetch code----------'&#13;
                git branch: 'release', credentialsId: 'git-credentials-backend', url: PROJECT_GIT_URL&#13;
            }&#13;
        }&#13;
        stage('fetch config') {&#13;
            agent {&#13;
                label 'node_xxx'	//在stage步骤中可以穿插在其他agent的操作步骤&#13;
            }&#13;
            steps {&#13;
                cleanWs() // 清理工作目录，防止工作目录产生其他产物&#13;
                dir('../jenkinsfile'){&#13;
                    echo '----------Fetch config----------'&#13;
                    git branch: 'master', credentialsId: 'git-credentials-backend', url: CONFIG_GIT_URL&#13;
                }&#13;
            }&#13;
        }&#13;
        stage('maven build'){&#13;
            steps {&#13;
                sh '/usr/local/maven3/bin/mvn -v'&#13;
                sh '/usr/local/maven3/bin/mvn clean package -Dmaven.test.skip=true -U'&#13;
            }&#13;
        }&#13;
        stage('docker build'){&#13;
            steps {&#13;
                sh '''&#13;
                    echo '当前时间为: ${env.TIMESTAMP}'&#13;
                    docker build -f ./Dockerfile -t ${DOCKER_IMAGE}:${env.TIMESTAMP} .&#13;
                '''&#13;
            }&#13;
        }&#13;
        stage('image push'){&#13;
            steps {&#13;
                sh '''&#13;
                    docker login -u ${DOCKER_REGISTER_CREDS_USR} -p=${DOCKER_REGISTER_CREDS_PSW} ${DOCKER_REGISTRY}&#13;
                    docker push ${DOCKER_IMAGE}:${env.TIMESTAMP}&#13;
                    docker rmi ${DOCKER_IMAGE}:${env.TIMESTAMP}&#13;
                '''&#13;
            }&#13;
        }&#13;
        stage('publish cn on k8s'){&#13;
            agent {&#13;
                label 'node_XXX'&#13;
            }&#13;
            steps {&#13;
                script {&#13;
                    // 运行kubectl命令，检查旧版本是否存在于集群中&#13;
                    def deploymentsName = 'bash -c 'kubectl get deployments -n ${K8S_NAMESPACE} ${CONTAINER_NAME} --output=jsonpath={.metadata.name}; exit 0;''&#13;
                    def result = sh(returnStdout: true, script: deploymentsName).trim()&#13;
                    echo '即将执行的服务: ${result}'&#13;
&#13;
                    if (result=='${CONTAINER_NAME}') {&#13;
                        // 旧版本已存在于集群中，执行kubectl set image更新镜像&#13;
                        echo '执行镜像滚动更新！'&#13;
                        sh '''&#13;
                            kubectl set image -n ${K8S_NAMESPACE} deployment/${CONTAINER_NAME} ${CONTAINER_NAME}=${DOCKER_IMAGE}:${env.TIMESTAMP} --record&#13;
                        '''&#13;
                    } else {&#13;
                        // 旧版本不存在于集群中，先执行kubectl apply部署&#13;
                        echo '服务不存在集群中，执行首次部署!'&#13;
                        sh '''&#13;
                            kubectl apply -f ../jenkinsfile/${PUB_ENV}/${K8S_NAMESPACE}/${YAML_NAME}&#13;
                            kubectl set image -n ${K8S_NAMESPACE} deployment/${CONTAINER_NAME} ${CONTAINER_NAME}=${DOCKER_IMAGE}:${env.TIMESTAMP} --record&#13;
                        '''&#13;
                    }&#13;
                }&#13;
            }&#13;
        }&#13;
        stage('Check Pod Status') {&#13;
            agent {&#13;
                label 'node_xxx'&#13;
            }&#13;
            steps {&#13;
                script {&#13;
                    def namespace = env.K8S_NAMESPACE&#13;
                    def containerName = env.CONTAINER_NAME&#13;
                    def timeoutSeconds = 120&#13;
&#13;
                    //等待滚动更新完成&#13;
                    sh 'kubectl rollout status deployment -n ${namespace} ${CONTAINER_NAME} --timeout=${timeoutSeconds}s'&#13;
                    sh 'sleep 5s'&#13;
                    // 获取Pod名称&#13;
                    def podName = sh(&#13;
                        script: 'kubectl get pods -l app=${containerName} -n ${namespace} -o jsonpath='{.items[0].metadata.name}'',&#13;
                        returnStdout: true&#13;
                    ).trim()&#13;
&#13;
                    // 检查容器状态&#13;
                    def containerReady = sh(&#13;
                        script: 'kubectl get pods ${podName} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].ready}'',&#13;
                        returnStdout: true&#13;
                    ).trim()&#13;
&#13;
                    if (containerReady == 'true') {&#13;
                        echo 'Container ${containerName} in Pod ${podName} is ready. ContainerIsReady:${containerReady}'&#13;
                    } else {&#13;
                        error 'Container ${containerName} in Pod ${podName} is not ready. ContainerIsReady:${containerReady}'&#13;
                    }&#13;
                }&#13;
            }&#13;
        }&#13;
    }&#13;
}&#13;
&#13;
```&#13;
&#13;
### 2.2 镜像发布到K8S&#13;
&#13;
*见 2.1中 `publish cn on k8s`步骤*&#13;
&#13;
### 2.3 滚动更新/更新后检测&#13;
&#13;
*见 2.1 中 `Check Pod Status` 步骤*。</description><guid isPermaLink="true">https://blog.witter.top/post/Jenkins-Kubernetes-xiang-jie-he.html</guid><pubDate>Sat, 22 Jun 2024 15:59:09 +0000</pubDate></item><item><title>Openssl 升级</title><link>https://blog.witter.top/post/Openssl%20-sheng-ji.html</link><description>## 升级支持TLS 1.3&#13;
&#13;
```shell&#13;
# 下载&#13;
wget https://www.openssl.org/source/openssl-3.0.13.tar.gz&#13;
# 解压&#13;
tar -zxvf openssl-3.0.13.tar.gz&#13;
# 配置、编译&#13;
./config  --prefix=/usr/local/openssl&#13;
make&#13;
# 检查是否出错&#13;
make test&#13;
# 安装&#13;
make install&#13;
```&#13;
&#13;
安装完成后检查链接库是否正常，将缺少的文件直接软链接到系统的`/usr/lib64`目录下&#13;
&#13;
```shell&#13;
cd /usr/local/openssl/bin&#13;
ldd openssl&#13;
&#13;
linux-vdso.so.1 =&gt;  (0x00007ffe3893b000)&#13;
libssl.so.3 =&gt; not found&#13;
libcrypto.so.3 =&gt; not found&#13;
libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007ff49e5e0000)&#13;
libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007ff49e3c4000)&#13;
libc.so.6 =&gt; /lib64/libc.so.6 (0x00007ff49dff6000)&#13;
/lib64/ld-linux-x86-64.so.2 (0x00007ff49e7e4000)&#13;
&#13;
ln -s /usr/local/openssl/lib64/libssl.so.3 /usr/lib64/libssl.so.3&#13;
ln -s /usr/local/openssl/lib64/libcrypto.so.3 /usr/lib64/libcrypto.so.3&#13;
&#13;
linux-vdso.so.1 =&gt;  (0x00007fffd2ddb000)&#13;
libssl.so.3 =&gt; /lib64/libssl.so.3 (0x00007fbc48fc8000)&#13;
libcrypto.so.3 =&gt; /lib64/libcrypto.so.3 (0x00007fbc48954000)&#13;
libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fbc48750000)&#13;
libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007fbc48534000)&#13;
libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fbc48166000)&#13;
/lib64/ld-linux-x86-64.so.2 (0x00007fbc4926d000)&#13;
```&#13;
&#13;
报错：&#13;
&#13;
```shell&#13;
Can't locate IPC/Cmd.pm in @INC (@INC contains: /home/sonkwo/openssl-3.0.13/util/perl /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 . /home/sonkwo/openssl-3.0.13/external/perl/Text-Template-1.56/lib) at /home/sonkwo/openssl-3.0.13/util/perl/OpenSSL/config.pm line 19.&#13;
BEGIN failed--compilation aborted at /home/sonkwo/openssl-3.0.13/util/perl/OpenSSL/config.pm line 19.&#13;
Compilation failed in require at /home/sonkwo/openssl-3.0.13/Configure line 23.&#13;
BEGIN failed--compilation aborted at /home/sonkwo/openssl-3.0.13/Configure line 23.&#13;
&#13;
yum install perl-IPC-Cmd&#13;
```&#13;
&#13;
。</description><guid isPermaLink="true">https://blog.witter.top/post/Openssl%20-sheng-ji.html</guid><pubDate>Thu, 20 Jun 2024 14:29:00 +0000</pubDate></item><item><title>Logrotate</title><link>https://blog.witter.top/post/Logrotate.html</link><description>logrotate 是一个 linux 系统日志的管理工具。</description><guid isPermaLink="true">https://blog.witter.top/post/Logrotate.html</guid><pubDate>Thu, 20 Jun 2024 14:27:25 +0000</pubDate></item><item><title>Kubernetes</title><link>https://blog.witter.top/post/Kubernetes.html</link><description>## 安装&#13;
&#13;
### 转发 IPv4 并让 iptables 看到桥接流量&#13;
&#13;
```shell&#13;
cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf&#13;
overlay&#13;
br_netfilter&#13;
EOF&#13;
&#13;
sudo modprobe overlay&#13;
sudo modprobe br_netfilter&#13;
&#13;
# 设置所需的 sysctl 参数，参数在重新启动后保持不变&#13;
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf&#13;
net.bridge.bridge-nf-call-iptables  = 1&#13;
net.bridge.bridge-nf-call-ip6tables = 1&#13;
net.ipv4.ip_forward                 = 1&#13;
EOF&#13;
&#13;
# 应用 sysctl 参数而不重新启动&#13;
sudo sysctl --system&#13;
```&#13;
&#13;
通过运行以下指令确认 `br_netfilter` 和 `overlay` 模块被加载：&#13;
&#13;
```bash&#13;
lsmod | grep br_netfilter&#13;
lsmod | grep overlay&#13;
```&#13;
&#13;
通过运行以下指令确认 `net.bridge.bridge-nf-call-iptables`、`net.bridge.bridge-nf-call-ip6tables` 和 `net.ipv4.ip_forward` 系统变量在你的 `sysctl` 配置中被设置为 1：&#13;
&#13;
```bash&#13;
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward&#13;
```&#13;
&#13;
### 临时关闭swap分区&#13;
&#13;
```shell&#13;
sudo swapoff -a&#13;
# Debian永久关闭&#13;
systemctl --type swap --all&#13;
systemctl mask dev-xxx.swap&#13;
```&#13;
&#13;
### 配置cgroup驱动&#13;
&#13;
```shell&#13;
# 备份/etc/containerd/config.toml&#13;
containerd config default &gt; /etc/containerd/config.toml&#13;
&#13;
SystemdCgroup = true&#13;
&#13;
[plugins.'io.containerd.grpc.v1.cri']&#13;
  sandbox_image = 'registry.aliyuncs.com/google_containers/pause:3.6'&#13;
```&#13;
&#13;
### 安装 kubeadm、kubelet 和 kubectl&#13;
&#13;
```shell&#13;
# 基于Red Hat的发行版&#13;
# 将 SELinux 设置为 permissive 模式（相当于将其禁用）&#13;
sudo setenforce 0&#13;
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config&#13;
&#13;
# 此操作会覆盖 /etc/yum.repos.d/kubernetes.repo 中现存的所有配置&#13;
cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF&#13;
[kubernetes]&#13;
name=Kubernetes&#13;
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64&#13;
enabled=1&#13;
gpgcheck=0&#13;
repo_gpgcheck=0&#13;
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg&#13;
EOF&#13;
&#13;
sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes&#13;
sudo systemctl enable --now kubelet&#13;
&#13;
# 基于Debian的发行版&#13;
sudo apt-get update&#13;
# apt-transport-https 可能是一个虚拟包（dummy package）；如果是的话，你可以跳过安装这个包&#13;
sudo apt-get install -y apt-transport-https ca-certificates curl&#13;
&#13;
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg&#13;
&#13;
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list&#13;
&#13;
sudo apt-get update&#13;
sudo apt-get install -y kubelet kubeadm kubectl&#13;
sudo apt-mark hold kubelet kubeadm kubectl&#13;
```&#13;
&#13;
### 添加指令补全&#13;
&#13;
```shell&#13;
sudo apt-get install bash-completion&#13;
echo 'source &lt;(kubectl completion bash)' &gt;&gt; ~/.bashrc&#13;
source ~/.bashrc&#13;
cat ~/.bashrc&#13;
```&#13;
&#13;
&#13;
&#13;
### 初始化Master节点&#13;
&#13;
控制平面节点是运行控制平面组件的机器， 包括 etcd（集群数据库） 和 API 服务器 （命令行工具 kubectl 与之通信）。</description><guid isPermaLink="true">https://blog.witter.top/post/Kubernetes.html</guid><pubDate>Thu, 20 Jun 2024 14:19:12 +0000</pubDate></item><item><title>Linux 工具</title><link>https://blog.witter.top/post/Linux%20-gong-ju.html</link><description># Linux工具&#13;
&#13;
## psmisc&#13;
&#13;
psmisc 是一个 Linux 工具集，它提供了一些管理和监视进程的实用工具。</description><guid isPermaLink="true">https://blog.witter.top/post/Linux%20-gong-ju.html</guid><pubDate>Thu, 20 Jun 2024 13:51:19 +0000</pubDate></item><item><title>Helm 安装</title><link>https://blog.witter.top/post/Helm%20-an-zhuang.html</link><description>[官方文档](https://helm.sh/zh/docs/intro/install/)&#13;
&#13;
### 用二进制版本安装&#13;
&#13;
每个Helm [[版本](https://github.com/helm/helm/releases)](https://github.com/helm/helm/releases)都提供了各种操作系统的二进制版本，这些版本可以手动下载和安装。</description><guid isPermaLink="true">https://blog.witter.top/post/Helm%20-an-zhuang.html</guid><pubDate>Thu, 20 Jun 2024 13:36:36 +0000</pubDate></item><item><title>Hadoop</title><link>https://blog.witter.top/post/Hadoop.html</link><description>&#13;
## 1.Hadoop概述&#13;
&#13;
**核心组件**&#13;
&#13;
**HDFS**（分布式文件系统）：解决海量数据存储&#13;
&#13;
**YARN**（作业调度和集群资源管理的框架）：解决资源任务调度&#13;
&#13;
**MAPREDUCE**（分布式运算编程框架）：解决海量数据计算&#13;
&#13;
**其他框架**&#13;
&#13;
| **框架**  | **用途**                                                  |&#13;
| --------- | --------------------------------------------------------- |&#13;
| HDFS      | 分布式文件系统                                            |&#13;
| MapReduce | 分布式运算程序开发框架                                    |&#13;
| ZooKeeper | 分布式协调服务基础组件                                    |&#13;
| HIVE      | 基于HADOOP的分布式数据仓库，提供基于SQL的查询数据操作     |&#13;
| FLUME     | 日志数据采集框架                                          |&#13;
| oozie     | 工作流调度框架                                            |&#13;
| Sqoop     | 数据导入导出工具（比如用于mysql和HDFS之间）               |&#13;
| Impala    | 基于hive的实时sql查询分析                                 |&#13;
| Mahout    | 基于mapreduce/spark/flink等分布式运算框架的机器学习算法库 |&#13;
&#13;
**默认端口更改**&#13;
&#13;
1. 在hadoop3.x之前，多个Hadoop服务的默认端口都属于Linux的临时端口范围（32768-61000）。</description><guid isPermaLink="true">https://blog.witter.top/post/Hadoop.html</guid><pubDate>Thu, 20 Jun 2024 13:35:27 +0000</pubDate></item><item><title>CloudFlare搭建Docker镜像源</title><link>https://blog.witter.top/post/CloudFlare-da-jian-Docker-jing-xiang-yuan.html</link><description>&gt; [示例网站](https://docker.3mz.cloudns.ch/) 使用的 [教程](https://blog.lty520.faith/%E5%8D%9A%E6%96%87/%E8%87%AA%E5%BB%BAdocker-hub%E5%8A%A0%E9%80%9F%E9%95%9C%E5%83%8F) 和 [仓库](https://github.com/Doublemine/container-registry-worker)，对仓库代码进行了部分自定义修改&#13;
&#13;
`docker.ts`&#13;
&#13;
```typescript&#13;
import HTML from './docker.html';&#13;
&#13;
export default {&#13;
  async fetch(request: Request): Promise&lt;Response&gt; {&#13;
    const url = new URL(request.url);&#13;
    const path = url.pathname;&#13;
    const originalHost = request.headers.get('host');&#13;
    const registryHost = 'registry-1.docker.io';&#13;
&#13;
    if (path.startsWith('/v2/')) {&#13;
      const headers = new Headers(request.headers);&#13;
      headers.set('host', registryHost);&#13;
&#13;
      const registryUrl = `https://${registryHost}${path}`;&#13;
      const registryRequest = new Request(registryUrl, {&#13;
        method: request.method,&#13;
        headers: headers,&#13;
        body: request.body,&#13;
        redirect: 'follow', // 按照教程修改了这一行&#13;
      });&#13;
&#13;
      const registryResponse = await fetch(registryRequest);&#13;
&#13;
      const responseHeaders = new Headers(registryResponse.headers);&#13;
      responseHeaders.set('access-control-allow-origin', originalHost as string);&#13;
      responseHeaders.set('access-control-allow-headers', 'Authorization');&#13;
      return new Response(registryResponse.body, {&#13;
        status: registryResponse.status,&#13;
        statusText: registryResponse.statusText,&#13;
        headers: responseHeaders,&#13;
      });&#13;
    } else {&#13;
      return new Response(HTML.replace(/{{host}}/g, originalHost as string), {&#13;
        status: 200,&#13;
        headers: {&#13;
          'content-type': 'text/html'&#13;
        }&#13;
      });&#13;
    }&#13;
  }&#13;
}&#13;
```&#13;
&#13;
## 如何部署&#13;
&#13;
```shell&#13;
# 使用wrangler&#13;
wrangler publish --config wrangler-dockerhub.toml&#13;
```&#13;
&#13;
## 如何自定义域名&#13;
&#13;
访问CloudFlare后台 -&gt; Workers和Pages -&gt; 进入需要自定义的Worker -&gt; 设置 -&gt; 触发器 -&gt; 自定义域&#13;
&#13;
![image-20240620140432250](https://img.witter.top/file/519f26de32e9f4b53cc3b.png)。</description><guid isPermaLink="true">https://blog.witter.top/post/CloudFlare-da-jian-Docker-jing-xiang-yuan.html</guid><pubDate>Thu, 20 Jun 2024 06:05:28 +0000</pubDate></item><item><title>EFK日志系统</title><link>https://blog.witter.top/post/EFK-ri-zhi-xi-tong.html</link><description>**注：现在新版本elastic stack已经支持将kafka作为输出和输入的目标**&#13;
&#13;
当日志不是结构化数据：*.log-&gt;filebeat-&gt;kafka-&gt;logstash-&gt;elasticsearch&#13;
&#13;
当日志是结构化数据：*.log-&gt;filebeat-&gt;kafka-&gt;filebeat-&gt;elasticsearch&#13;
&#13;
区别在于是否需要logstash进行日志的过滤和结构化；&#13;
&#13;
## 结构化日志&#13;
&#13;
**连接示例配置文件**&#13;
&#13;
### filebeat 1&#13;
&#13;
&gt; 此filebeat实例为日志采集端，即kafka生产者，可以有多个实例进行采集&#13;
&#13;
```yaml&#13;
filebeat.inputs:&#13;
- type: filestream #新版本常用文件流输入方式&#13;
  id: my-filestream-id #每个文件流输入必须有一个唯一的 ID。</description><guid isPermaLink="true">https://blog.witter.top/post/EFK-ri-zhi-xi-tong.html</guid><pubDate>Thu, 20 Jun 2024 03:26:28 +0000</pubDate></item><item><title>Azkaban</title><link>https://blog.witter.top/post/Azkaban.html</link><description>## 1.部署&#13;
&#13;
### 1.1 Solo&#13;
&#13;
独立服务器是Azkaban的独立实例，也是最简单的开始。</description><guid isPermaLink="true">https://blog.witter.top/post/Azkaban.html</guid><pubDate>Thu, 20 Jun 2024 02:47:18 +0000</pubDate></item><item><title>Certbot 申请SSL证书</title><link>https://blog.witter.top/post/Certbot%20-shen-qing-SSL-zheng-shu.html</link><description>## 官方网址&#13;
&#13;
https://certbot.eff.org/&#13;
&#13;
## 申请SSL证书&#13;
&#13;
&gt; 指定Nginx路径方法：&#13;
&gt;&#13;
&gt; 方式1： certbot --nginx    (当linux有多个版本nginx，会出现找错nginx的配置文件路径)&#13;
&gt;&#13;
&gt; 方式2： certbot --nginx-server-root  /usr/local/nginx/conf    (指定nginx的配置文件路径)&#13;
&#13;
1. 为单域名申请SSL证书&#13;
&#13;
   ```shell&#13;
   # 安装 certbot 以及 certbot nginx 插件&#13;
   sudo yum install certbot python2-certbot-nginx -y&#13;
   &#13;
   # 执行配置，中途会询问你的邮箱，如实填写即可&#13;
   sudo certbot --nginx&#13;
   &#13;
   # 自动续约&#13;
   sudo certbot renew --dry-run&#13;
   &#13;
   # 获得并安装一个证书。</description><guid isPermaLink="true">https://blog.witter.top/post/Certbot%20-shen-qing-SSL-zheng-shu.html</guid><pubDate>Thu, 20 Jun 2024 02:44:24 +0000</pubDate></item><item><title>V's first</title><link>https://blog.witter.top/post/V%27s%20first.html</link><description>&gt; 所做留下痕迹&#13;
&#13;
```&#13;
&gt; [!NOTE]&#13;
&gt; Useful information that users should know, even when skimming content.&#13;
&#13;
&gt; [!TIP]&#13;
&gt; Helpful advice for doing things better or more easily.&#13;
&#13;
&gt; [!IMPORTANT]&#13;
&gt; Key information users need to know to achieve their goal.&#13;
&#13;
&gt; [!WARNING]&#13;
&gt; Urgent info that needs immediate user attention to avoid problems.&#13;
&#13;
&gt; [!CAUTION]&#13;
&gt; Advises about risks or negative outcomes of certain actions.&#13;
```&#13;
&#13;
&gt; [!NOTE]&#13;
&gt; Useful information that users should know, even when skimming content.&#13;
&#13;
&gt; [!TIP]&#13;
&gt; Helpful advice for doing things better or more easily.&#13;
&#13;
&gt; [!IMPORTANT]&#13;
&gt; Key information users need to know to achieve their goal.&#13;
&#13;
&gt; [!WARNING]&#13;
&gt; Urgent info that needs immediate user attention to avoid problems.&#13;
&#13;
&gt; [!CAUTION]&#13;
&gt; Advises about risks or negative outcomes of certain actions.。</description><guid isPermaLink="true">https://blog.witter.top/post/V%27s%20first.html</guid><pubDate>Thu, 20 Jun 2024 02:19:02 +0000</pubDate></item><item><title>Python 小记</title><link>https://blog.witter.top/post/Python%20-xiao-ji.html</link><description>## 1.初识Python&#13;
&#13;
1989 年，作者决心开发一个新的解释程序&#13;
&#13;
1991年，第一个Python解释器诞生&#13;
&#13;
## 2.安装Python&#13;
&#13;
官网下载：https://www.python.org/downloads/&#13;
&#13;
- 添加到path&#13;
- 修改默认路径&#13;
&#13;
cmd 输入python&#13;
&#13;
```shell&#13;
C:\Users\Lenovo&gt;python&#13;
Python 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)] on win32&#13;
Type 'help', 'copyright', 'credits' or 'license' for more information.&#13;
&gt;&gt;&gt;&#13;
```&#13;
&#13;
**Python解释器**：用来翻译Python代码，并提交给计算机执行。</description><guid isPermaLink="true">https://blog.witter.top/post/Python%20-xiao-ji.html</guid><pubDate>Thu, 30 May 2024 11:27:27 +0000</pubDate></item><item><title>你觉得这是什么就是什么</title><link>https://blog.witter.top/post/ni-jue-de-zhe-shi-shen-me-jiu-shi-shen-me.html</link><description>[image](https://www.3mz.cloudns.ch/file/8859a6285fa21a73afc4c.jpg)&#13;
&lt;!-- ##{'timestamp':1705817030}## --&gt;。</description><guid isPermaLink="true">https://blog.witter.top/post/ni-jue-de-zhe-shi-shen-me-jiu-shi-shen-me.html</guid><pubDate>Sun, 21 Jan 2024 06:03:50 +0000</pubDate></item><item><title>About</title><link>https://blog.witter.top/about.html</link><description>- 人可以没有傲气，但不能没有傲骨；&#13;
- 兴趣是学习最好的老师；&#13;
&#13;
&#13;
&lt;span id='busuanzi'&gt;:robot:感谢第&lt;span&gt;&lt;/span&gt;小伙伴的&lt;span&gt;&lt;/span&gt;次访问此页面。</description><guid isPermaLink="true">https://blog.witter.top/about.html</guid><pubDate>Thu, 20 Jun 2024 14:43:02 +0000</pubDate></item><item><title>自存链接</title><link>https://blog.witter.top/link.html</link><description>- [图床](https://www.3mz.cloudns.ch)&#13;
- [短链](https://surl.witter.top)&#13;
- [监控](https://monit.witter.top)&#13;
- [docker镜像源](https://docker.3mz.cloudns.ch/)&#13;
- [丑丑头像生成器](https://txstc55.github.io/ugly-avatar)&#13;
- [People Die, but Long Live GitHub](https://laike9m.com/blog/people-die-but-long-live-github,122/)&#13;
- [Pikimov | 网页版 AE](https://pikimov.com/)&#13;
- [Dropbase | Prompt 和 Python 开发 Web 应用工具](https://github.com/DropbaseHQ/dropbase)&#13;
- [Logdy | 便捷日志查看工具](https://logdy.dev/)&#13;
- [Pezzo | 开源 LLMOps 平台](https://github.com/pezzolabs/pezzo)&#13;
- [WASM 视频处理工具](https://www.videozip.online/)&#13;
- [手写体文稿生成器](https://vtool.pro/handwriting/index.html)&#13;
- [组建自己的 AI 集群](https://github.com/exo-explore/exo)&#13;
- [一键视频换脸](https://github.com/hacksider/Deep-Live-Cam)&#13;
&#13;
&lt;span id='busuanzi'&gt;:robot:感谢第&lt;span&gt;&lt;/span&gt;小伙伴的&lt;span&gt;&lt;/span&gt;次访问此页面。</description><guid isPermaLink="true">https://blog.witter.top/link.html</guid><pubDate>Fri, 21 Jun 2024 06:35:57 +0000</pubDate></item></channel></rss>